{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "UNet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbSvE6h4mZyO"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQX7R4bhZy5h"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3wQUzRGYUMm"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDEza-tcR4SQ"
      },
      "source": [
        "# If you hit a problem with checksums, you can execute the following line first\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=oxford_iiit_pet:3.1.0\n",
        "\n",
        "# download the dataset and get info\n",
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKIvihxy4q0K"
      },
      "source": [
        "Let's briefly examine the contents of the dataset you just downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLqGeTL39uLz"
      },
      "source": [
        "print(dataset.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfCgOKbLYdH8"
      },
      "source": [
        "### Information about the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpedAXHz-Qwl"
      },
      "source": [
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gdL7Ui5XCLW"
      },
      "source": [
        "## Preprocessing utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIVLU-WzXBLf"
      },
      "source": [
        "# Random flipping the image and the mask\n",
        "def random_flip(input_image, input_mask):\n",
        "  \n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    input_mask = tf.image.flip_left_right(input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev5J1-bqXKOT"
      },
      "source": [
        "#   Normalizes the input image pixel values to be from [0,1].\n",
        "#   Subtracts 1 from the mask labels to have a range from [0,2]\n",
        "def normalize(input_image, input_mask):\n",
        "  \n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k5oNw81XOPj"
      },
      "source": [
        "# Resizes, normalizes, and flips the training data\n",
        "@tf.function\n",
        "def load_image_train(datapoint):\n",
        "\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n",
        "  input_image, input_mask = random_flip(input_image, input_mask)\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "  \n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yXDjfhlXSL0"
      },
      "source": [
        "# Resizes and normalizes the test data\n",
        "def load_image_test(datapoint):\n",
        "  \n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwxJaYLaSPb"
      },
      "source": [
        "### Preprocessing the train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39fYScNz9lmo"
      },
      "source": [
        "train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test = dataset['test'].map(load_image_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YzjUQ9XanLu"
      },
      "source": [
        "### Preparing batches for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeFwFDN6EVoI"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# shuffle and group the train set into batches\n",
        "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "# do a prefetch to optimize processing\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# group the test set into batches\n",
        "test_dataset = test.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNbqReTIXcqx"
      },
      "source": [
        "## Visualization Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIS21cpaXjte"
      },
      "source": [
        "# Displays a list of images/masks and overlays a list of IOU and Dice Scores\n",
        "def display_with_metrics(display_list, iou_list, dice_score_list):\n",
        "  \n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "  \n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\\n\".join(display_string_list)\n",
        "\n",
        "  display(display_list, [\"Image\", \"Predicted Mask\", \"True Mask\"], display_string=display_string) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n34OGwJXzFEu"
      },
      "source": [
        "# Displays a list of images/masks\n",
        "def display(display_list,titles=[], display_string=None):\n",
        "\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(titles[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    if display_string and i == 1:\n",
        "      plt.xlabel(display_string, fontsize=12)\n",
        "    img_arr = tf.keras.preprocessing.image.array_to_img(display_list[i])\n",
        "    plt.imshow(img_arr)\n",
        "  \n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31pX8HgPWwjX"
      },
      "source": [
        "# Displays the first image and its mask from a dataset\n",
        "def show_image_from_dataset(dataset, n):\n",
        "\n",
        "  for image, mask in dataset.take(n):\n",
        "    sample_image, sample_mask = image, mask\n",
        "  display([sample_image, sample_mask], titles=[\"Image\", \"True Mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INsUIMfnWwgV"
      },
      "source": [
        "# Plots a given metric from the model history\n",
        "def plot_metrics(metric_name, title, ylim=5):\n",
        "  \n",
        "  plt.title(title)\n",
        "  plt.ylim(0,ylim)\n",
        "  plt.plot(model_history.history[metric_name],color='blue',label=metric_name)\n",
        "  plt.plot(model_history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwuW88nUWwck"
      },
      "source": [
        "# Class list of the mask pixels\n",
        "class_names = ['pet', 'background', 'outline']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa3gMAE_9qNa"
      },
      "source": [
        "### Sample images and their correponding masks from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6u_Rblkteqb"
      },
      "source": [
        "# Display an image from the train set\n",
        "show_image_from_dataset(train, 3)\n",
        "\n",
        "# Display an image from the test set\n",
        "show_image_from_dataset(test, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFLaRrxSchXN"
      },
      "source": [
        "## Encoder Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92pGoq6rWTMo"
      },
      "source": [
        "# Adds 2 convolutional layers with the parameters passed to it\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size = 3):\n",
        "\n",
        "  x = input_tensor\n",
        "  for i in range(2):\n",
        "    x = tf.keras.layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
        "            kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "  \n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev88wbXlWXQu"
      },
      "source": [
        "# Adds two convolutional blocks and then perform down sampling on output of convolutions\n",
        "def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n",
        "\n",
        "  f = conv2d_block(inputs, n_filters=n_filters)\n",
        "  p = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(f)\n",
        "  p = tf.keras.layers.Dropout(0.3)(p)\n",
        "\n",
        "  return f, p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoGZBIzs8Ln-"
      },
      "source": [
        "# Final Encoder\n",
        "def encoder(inputs):\n",
        "\n",
        "  f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3)\n",
        "  f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=0.3)\n",
        "  f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=0.3)\n",
        "  f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=0.3)\n",
        "\n",
        "  return p4, (f1, f2, f3, f4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6lSYsoOc6j6"
      },
      "source": [
        "### Bottleneck\n",
        "\n",
        "\n",
        "A bottleneck follows the encoder block and is used to extract more features. This does not have a pooling layer so the dimensionality remains the same. You can use the `conv2d_block()` function defined earlier to implement this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLzUf31Cuh-f"
      },
      "source": [
        "# Bottleneck convolutions to extract more features before the upsampling layers\n",
        "def bottleneck(inputs):\n",
        "  \n",
        "  bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
        "\n",
        "  return bottle_neck"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwXnPQeWdg7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LpbeWrdFN9"
      },
      "source": [
        "## Decoder Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4DcNAasWdaQ"
      },
      "source": [
        "# One decoder block of the UNet\n",
        "def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):\n",
        "\n",
        "  u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides = strides, padding = 'same')(inputs)\n",
        "  c = tf.keras.layers.concatenate([u, conv_output])\n",
        "  c = tf.keras.layers.Dropout(dropout)(c)\n",
        "  c = conv2d_block(c, n_filters, kernel_size=3)\n",
        "\n",
        "  return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XACX8TJh1oKd"
      },
      "source": [
        "# Final Decoder of the UNet chaining together 4 decoder blocks\n",
        "def decoder(inputs, convs, output_channels):\n",
        "  \n",
        "  f1, f2, f3, f4 = convs\n",
        "  c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAtWsYwGExtB"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "You can finally build the UNet by chaining the encoder, bottleneck, and decoder. You will specify the number of output channels and in this particular set, that would be `3`. That is because there are three possible labels for each pixel: 'pet', 'background', and 'outline'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gE1jiz5u6Zg"
      },
      "source": [
        "# Final UNet architecture\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def unet():\n",
        "\n",
        "  # specify the input shape\n",
        "  inputs = tf.keras.layers.Input(shape=(128, 128,3,))\n",
        "\n",
        "  # feed the inputs to the encoder\n",
        "  encoder_output, convs = encoder(inputs)\n",
        "\n",
        "  # feed the encoder output to the bottleneck\n",
        "  bottle_neck = bottleneck(encoder_output)\n",
        "\n",
        "  # feed the bottleneck and encoder block outputs to the decoder\n",
        "  # specify the number of classes via the `output_channels` argument\n",
        "  outputs = decoder(bottle_neck, convs, output_channels=OUTPUT_CHANNELS)\n",
        "  \n",
        "  # create the model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "# instantiate the model\n",
        "model = unet()\n",
        "\n",
        "# see the resulting model architecture\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2VoicVLhDi-"
      },
      "source": [
        "# Plotting the complete model architecture\n",
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    to_file=\"model.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CsuvlqRd6PY"
      },
      "source": [
        "### Compile the model and configure the optimizer, loss and metrics for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEyXtFjCzZv5"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXnE5tcqeFQL"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StKDH_B9t4SD"
      },
      "source": [
        "# Configure the training parameters and train the model\n",
        "\n",
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "EPOCHS = 100\n",
        "VAL_SUBSPLITS = 5\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "# this will take around 20 minutes to run\n",
        "model_history = model.fit(train_dataset, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDXdaWC1eI2b"
      },
      "source": [
        "### Plotting the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mu0SAbt40Q"
      },
      "source": [
        "plot_metrics(\"loss\", title=\"Training vs Validation Loss\", ylim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA1lgjA7i3Ff"
      },
      "source": [
        "### Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mujno1tBi17I"
      },
      "source": [
        "model.save(\"trained_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unP3cnxo_N72"
      },
      "source": [
        "## Prediction Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuHPDMC1yYGB"
      },
      "source": [
        "# Unpacks the test dataset and returns the input images and segmentation masks\n",
        "def get_test_image_and_annotation_arrays():\n",
        "\n",
        "  ds = test_dataset.unbatch()\n",
        "  ds = ds.batch(info.splits['test'].num_examples)\n",
        "  \n",
        "  images = []\n",
        "  y_true_segments = []\n",
        "\n",
        "  for image, annotation in ds.take(1):\n",
        "    y_true_segments = annotation.numpy()\n",
        "    images = image.numpy()\n",
        "  \n",
        "  y_true_segments = y_true_segments[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))]\n",
        "  \n",
        "  return images[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))], y_true_segments\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tifrCpOfJXp"
      },
      "source": [
        "# Creates the segmentation mask by getting the channel with the highest probability\n",
        "def create_mask(pred_mask):\n",
        "\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  \n",
        "  return pred_mask[0].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDiKN2g4fJPe"
      },
      "source": [
        "# Returns the predicted mask from the input image\n",
        "def make_predictions(image, mask, num=1):\n",
        "\n",
        "  image = np.reshape(image,(1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  pred_mask = model.predict(image)\n",
        "  pred_mask = create_mask(pred_mask)\n",
        "\n",
        "  return pred_mask "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3hfZSSIwi1y"
      },
      "source": [
        "def class_wise_metrics(y_true, y_pred):\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothening_factor = 0.00001\n",
        "  for i in range(3):\n",
        "    \n",
        "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "    \n",
        "    iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
        "    class_wise_iou.append(iou)\n",
        "    \n",
        "    dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n",
        "    class_wise_dice_score.append(dice_score)\n",
        "\n",
        "  return class_wise_iou, class_wise_dice_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEV6XHFoCDTa"
      },
      "source": [
        "# Setup the ground truth and predictions.\n",
        "# Get the ground truth from the test set\n",
        "y_true_images, y_true_segments = get_test_image_and_annotation_arrays()\n",
        "\n",
        "# feed the test set to th emodel to get the predicted masks\n",
        "results = model.predict(test_dataset, steps=info.splits['test'].num_examples//BATCH_SIZE)\n",
        "results = np.argmax(results, axis=3)\n",
        "results = results[..., tf.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNcC3lBtwnsY"
      },
      "source": [
        "# Compute the class wise metrics\n",
        "cls_wise_iou, cls_wise_dice_score = class_wise_metrics(y_true_segments, results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgd1hmRK3mdp"
      },
      "source": [
        "# Show the IOU for each class\n",
        "for idx, iou in enumerate(cls_wise_iou):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, iou)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECFMjlw63nHb"
      },
      "source": [
        "# Show the Dice Score for each class\n",
        "for idx, dice_score in enumerate(cls_wise_dice_score):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAN56XW9zueE"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xzI28AfxFQi"
      },
      "source": [
        "# Please input a number between 0 to 3647 to pick an image from the dataset\n",
        "integer_slider = 2700\n",
        "\n",
        "# Get the prediction mask\n",
        "y_pred_mask = make_predictions(y_true_images[integer_slider], y_true_segments[integer_slider])\n",
        "\n",
        "# Compute the class wise metrics\n",
        "iou, dice_score = class_wise_metrics(y_true_segments[integer_slider], y_pred_mask)  \n",
        "\n",
        "# Overlay the metrics with the images\n",
        "display_with_metrics([y_true_images[integer_slider], y_pred_mask, y_true_segments[integer_slider]], iou, dice_score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}